# Part 1 : Design for streaming analytics
## Question 1

### Dataset

The dataset I will use for this assignment will be the one of bts.
This is a collection of sensors data from base stations. The data structure is as follow:

* ```station_id```: the id of the stations
* ```datapoint_id```: the id of the sensor (data point)
* ```alarm_id```: the id of the alarm
* ```event_time```: the time at which the event occurs
* ```value```: the value of the measurement of the datapoint
* ```valueThreshold```: the threshold set for the alarm. Note that some threshold values are set to a default value of 999999.
* ```isActive```: the alarm is active (true ) or not (false)
* ```storedtime```: no store

I find this dataset particularly suitable for this assignment because of its timestamp which is a temporal placement, very useful since the dataset contains records generated by sensors.
By checking it, I could realize that it is a simple and clean database (with only the last empty column in the csv files) unlike the google dataset I took for the previous two assignments and which I had to clean some data.
Bts is a sensors' dataset, which is therefore very well for a near-real time ingestion flux and also for analytics since most of the analytics documentation presented on the internet often takes sensors as an example.

### Stream
For a given station we can keep track of minimum, maximum and mean values for each sensor. 
In addition, streaming analysis will be used mainly for predictive maintenance and reliability analysis in the case of a sensor dataset (in an IoT environment)
As it allows an near-real time analysis, it allows to have a very short reaction time to a possible problem that could occur.

* ```frequency analysis``` : check that it is coherent and stable
  * example: ```anomaly detection diagnosis``` : automatically remove noise/incoherent data (peak of a value whereas before and after the data are stable and the measurements are very close in time)
* ```reliability analytic```: estimate the failure probability and predict future alarms
  * example: if the temperature starts to rise slowly and approaches the maximum threshold we can predict and launch an imminent overheating alert
* detect that the alarm is currently on (isActive is true) (and detect where that alarm is)
* send back in the pipe (another one) to the user the data of the real-time analysis


### Batch

Batch analytics will serve more to perform summaries, for example we can compute metrics like :
* Rate distribution data by station (which station issued the most data)
* For a station, make a histogram of the temperature over a day (over a month, over a year...)
* Look at the number of alarms triggered per day/month/... per station
* Look at the type of alarm that occurs most often (the alarm that exceeds the threshold the most)
* For an alarm type: at which time of the day it is most triggered

## Question 2

### Keyed and Non-Keyed data streams
From what I understood from the documentation on the Internet that I was able to read, the main difference between keyed and non-keyed data streams is that in the case of keyed streams, any attributes of the incoming events can be used as a key and each logical keyed stream can be processed in parallel from the rest. In the case of non-keyed streams, each stream will be perform by a single task (not split into parallel process).

To make things easier, I will arbitrarily take a basic schema in which each user will correspond to a single station. There will already be a distribution per station and in this case handle keyed will not be necessarily necessary.

However, to do analytics on our dataset, it would be more convenient to be able to sort more easily by other factors than the stations (for example, to be able to sort by alarm) and it is possible to see a utility in the handle keyed that would save time by parallelizing processes.

So I will handle keyed data streams.

### Delivery guarantees

There are several guarantees that we could provide to the client in this type of assignment. I have selected some of them that I think are particularly relevant here, such as :

* ```Order of file arrival``` : for the alert triggering the order is important
  * if in the received data, we have a data indicating that there is no alert with an event_time at 11:30 am but the following one indicates that there is one with an event_time at 11:25 am, it is not necessary to trigger an alert since the alarm is over
* ```Availability of the server``` : server fully available depending on the duplication rate (for example, on the [Microsoft Azure's website](https://azure.microsoft.com/en-in/support/legal/sla/summary/) they guarantee at least 99.9% availability of the Azure Active Directory Basic and Premium services to their customers).
* ```Event delivery guarantees``` : My dataset being made up of sensor data, we are here in the heart of the IoT. In the IoT, we often encounter cases where there is a lot of data and not necessarily a good bandwidth from the point of view of the embedded object (here from the sensor point of view). In this case it is probably preferable to lose a data from time to time in favour of speed of execution. That's why I think that MQTT's Event Delivery Guarantee default value: ```At Most Once``` is the best option in our case.

***************************************
https://admhelp.microfocus.com/lr/en/12.60-12.63/help/WebHelp/Content/Controller/mon_mqtt.htm

https://docs.microsoft.com/en-us/stream-analytics-query/event-delivery-guarantees-azure-stream-analytics

***********************************

## Question 3

### Types of time should be associated with stream sources
In our dataset, we are lucky to have a timestamp (event_time), which we can easily keep to associate it with the stream source for the analytics.

### Types of windows

We could have, just like not having a window. If it is a stream analysis where there is triggers listening some trigger events, there is no need for a window.

We can have windows in number of rows or time windows.

The advantages of a window by number of rows is that there is always the same number of data for the analysis, which means the same reliability rate but the time coherence can be biased in case of a too large gap between two rows for example.

If we take a time window, there may be a reliability rate varying between windows depending on the number of rows present in each time gap. However, this allows a better coherence between the windows and it seems closer to reality.

The choice between a time window or by number of rows can be made on a case-by-case basis depending on the data sent by the sensors and the customer's wishes.

In addition, another aspect of the window is its ability to be continuously close to the present time. The stream flow being a continuous flow, it seems to me essential that the window we choose is a sliding window.

***************************
Pour analyse/a quel point les donnees sont fiables -- pour tout ce qui est filtering

en time window : si grand gap entre chaque row, le taux de fiabilite peut etre moins fiable mais ca semble plus proche de la realite
Ca peut etre au cas par cas en fonction des datas envoyes par les capteurs et des envies du client

avec keyed on peut regler la window !!!!!!!!!
***************************

## Question 4

There is 2 types of metrics here : the ones directly or indirectly related to the customer's data and the ones that are not related at all to the customer's data.

For the metrics not related at all to the customer's data, we can have, for example, the ```server response time``` which is the average time the server takes to response to a query.


INSERT LIENS

I thought about several metrics that we could use here :

* ```Data Conversion Errors``` : which is the number of data that could not be converted to the expected output schema. This scenario will result in the loss of data (Drop)

* ```Out-of-Order Events``` : which in our case could include events for which the time of the event_time is later than the current time or when the data does not arrive in the right order (for example when the event_time of the last event recorded is after the event_time of the event we want to record). This scenario will result in the loss of data (Drop) because in IoT it is often not the number of data that is missing but rather the quality of the data and in such cases, we may wonder about the quality of the data. So, the option to drop the event appears to me to be the least bad.

* ```Input Deserialization Errors``` : which is the number of input events that could not be deserialized.

* ```Input Event Bytes``` : which is the amount of data received by the stream analytics job, in bytes.

* ```Ingestion speed``` : which is the average ingestion rate for an event.

* ```Number of returned analytics``` : which is the number of analytics that was send to the client.

* ```Records``` : which is the number of records read or written (respectively per input or output stream).

* ```Dropped records``` : The number of records that were dropped (for any possible reason).

* ```Delay in the queue``` : Indicates how far from the current time an application is reading from the streaming source ( in other words, the difference between the insertion time in the queue of the current event and the insertion time in the queue of the last one of the queue, the insertion time in the queue can normally be recovered via metadata)

**************************************
Vitesse de l'analyse pour une ligne
Vitesse d'insertion max
Nombre de row qui engendre un retour utilisateur (

https://docs.microsoft.com/en-us/azure/stream-analytics/stream-analytics-monitoring



https://docs.aws.amazon.com/kinesisanalytics/latest/dev/monitoring-metrics.html


Max delay : Temps de la trame qui a pris le plus longtemps a etre traite

runtime error ?
*********************************

## Question 5

client provide streamapp --> suit notre modele
streamapp --> analytic --> store data in db

client a un producer, le producer du client envoie les datas dans un channel

client peut toogle (activer/desactiver) consumer du server

flink (args = pipe + script) : applique sur le pipe le script, sans regarder veritablement le pipe
store usefull data for analytics and store metrics


fichier python chez client --> stream sur rabbitmq --> client lance script sur Flink qui lit le stream --> le transforme en objet java : BTSAlarmAlert & BTSAlarmEvent --> Analytics avec objet java --> retourne analytics en stream --> client lit le retour de l'analytics


# Part 2

## Question 1

tel script python qui prend en entree fichier csv (prendrait des donnees en temps reel dans un environnement reel) converti les donnees selon tel format --> Flink converti le stream recu (string convertion

